{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL-CAI Training Notebook (Fixed for Stacked LoRA)\n",
    "\n",
    "This notebook implements the Reinforcement Learning stage of Constitutional AI (RL-CAI) with proper support for stacked LoRA models.\n",
    "\n",
    "Steps:\n",
    "1. Generate preference data using SL-CAI models (HM7B + CAI LoRA)\n",
    "2. Train reward models on constitutional preferences\n",
    "3. Fine-tune with PPO using constitutional feedback\n",
    "\n",
    "**Important**: Run the cells in order, starting with Section 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites - Run This First!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for persistent storage\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set up paths\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/Constitutional_AI_Project'\n",
    "PROJECT_DIR = '/content/Constitutional_AI_Project'\n",
    "GITHUB_REPO = 'https://github.com/ychleee/Constitutional_AI_Project.git'\n",
    "\n",
    "# Clone or update repository\n",
    "if not os.path.exists(PROJECT_DIR):\n",
    "    print('ðŸ“¥ Cloning repository...')\n",
    "    !git clone {GITHUB_REPO} {PROJECT_DIR}\n",
    "else:\n",
    "    print('ðŸ“¥ Updating repository...')\n",
    "    !cd {PROJECT_DIR} && git pull origin main\n",
    "\n",
    "# Add project to Python path\n",
    "sys.path.append(PROJECT_DIR)\n",
    "\n",
    "# Install required dependencies\n",
    "print('ðŸ“¦ Installing dependencies...')\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q peft>=0.6.0 trl>=0.7.0 bitsandbytes>=0.41.0\n",
    "!pip install -q einops tensorboard wandb safetensors\n",
    "!pip install -q jsonlines pandas numpy scikit-learn matplotlib seaborn tqdm rich\n",
    "\n",
    "# Create necessary directories in Drive\n",
    "directories = [\n",
    "    f'{DRIVE_PROJECT_PATH}/data/red_team',\n",
    "    f'{DRIVE_PROJECT_PATH}/data/helpfulness',\n",
    "    f'{DRIVE_PROJECT_PATH}/data/sl_datasets',\n",
    "    f'{DRIVE_PROJECT_PATH}/data/rl_datasets',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/hm7b',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/hm7b_deontological',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/hm7b_consequentialist',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/deontological/reward_model',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/deontological/rl_cai',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/consequentialist/reward_model',\n",
    "    f'{DRIVE_PROJECT_PATH}/models/consequentialist/rl_cai',\n",
    "    f'{DRIVE_PROJECT_PATH}/results/rl_training_logs'\n",
    "]\n",
    "\n",
    "for dir_path in directories:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print('âœ… Prerequisites complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/Constitutional_AI_Project'\n",
    "PROJECT_DIR = '/content/Constitutional_AI_Project'\n",
    "DATA_PATH = f\"{DRIVE_PROJECT_PATH}/data\"\n",
    "MODEL_PATH = f\"{DRIVE_PROJECT_PATH}/models\"\n",
    "\n",
    "# Model paths for stacked LoRA architecture\n",
    "BASE_MODEL_NAME = \"mistralai/Mistral-7B-v0.1\"  # Base Mistral model\n",
    "HM7B_PATH = f\"{MODEL_PATH}/hm7b\"  # HM7B LoRA adapter\n",
    "DEONT_CAI_PATH = f\"{MODEL_PATH}/hm7b_deontological\"  # Deontological CAI LoRA\n",
    "CONSEQ_CAI_PATH = f\"{MODEL_PATH}/hm7b_consequentialist\"  # Consequentialist CAI LoRA\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_info = torch.cuda.get_device_properties(0)\n",
    "    print(f\"âœ… GPU: {gpu_info.name}\")\n",
    "    print(f\"   Memory: {gpu_info.total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected!\")\n",
    "\n",
    "# Configuration based on GPU\n",
    "if \"A100\" in gpu_info.name:\n",
    "    CONFIG = {\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation\": 4,\n",
    "        \"max_length\": 1024,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"ppo_batch_size\": 8,\n",
    "        \"mini_batch_size\": 2\n",
    "    }\n",
    "else:  # T4 or other GPUs\n",
    "    CONFIG = {\n",
    "        \"batch_size\": 1,\n",
    "        \"gradient_accumulation\": 8,\n",
    "        \"max_length\": 512,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"ppo_batch_size\": 4,\n",
    "        \"mini_batch_size\": 1\n",
    "    }\n",
    "\n",
    "print(f\"\\nConfiguration: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Stacked LoRA Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "def load_stacked_lora_model(constitution_type, device_map=\"auto\"):\n",
    "    \"\"\"\n",
    "    Load a stacked LoRA model: Base Mistral -> HM7B LoRA -> CAI LoRA\n",
    "    \n",
    "    Args:\n",
    "        constitution_type: 'deontological' or 'consequentialist'\n",
    "        device_map: Device mapping for model\n",
    "    \n",
    "    Returns:\n",
    "        model: The loaded model with stacked LoRA\n",
    "        tokenizer: The tokenizer\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ”„ Loading {constitution_type} model with stacked LoRA...\")\n",
    "    \n",
    "    # Step 1: Load base Mistral model\n",
    "    print(\"  Loading base Mistral-7B...\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device_map,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: Load and merge HM7B LoRA\n",
    "    if os.path.exists(HM7B_PATH):\n",
    "        print(\"  Loading HM7B LoRA adapter...\")\n",
    "        hm7b_model = PeftModel.from_pretrained(base_model, HM7B_PATH)\n",
    "        print(\"  Merging HM7B LoRA...\")\n",
    "        merged_model = hm7b_model.merge_and_unload()\n",
    "        del hm7b_model\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(\"  âš ï¸ HM7B not found, using base model\")\n",
    "        merged_model = base_model\n",
    "    \n",
    "    # Step 3: Load CAI LoRA on top\n",
    "    cai_path = DEONT_CAI_PATH if constitution_type == \"deontological\" else CONSEQ_CAI_PATH\n",
    "    if os.path.exists(cai_path):\n",
    "        print(f\"  Loading {constitution_type} CAI LoRA adapter...\")\n",
    "        final_model = PeftModel.from_pretrained(merged_model, cai_path)\n",
    "    else:\n",
    "        print(f\"  âš ï¸ {constitution_type} CAI LoRA not found at {cai_path}\")\n",
    "        # Try alternative path (from SL training)\n",
    "        alt_path = f\"{MODEL_PATH}/{constitution_type}/sl_cai\"\n",
    "        if os.path.exists(alt_path):\n",
    "            print(f\"  Using SL-CAI model from {alt_path}\")\n",
    "            final_model = PeftModel.from_pretrained(merged_model, alt_path)\n",
    "        else:\n",
    "            print(f\"  Using HM7B model without CAI LoRA\")\n",
    "            final_model = merged_model\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"âœ… Loaded {constitution_type} model\")\n",
    "    return final_model, tokenizer\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.preference_generator import PreferenceGenerator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_preferences_for_constitution(constitution_type, num_prompts=20):\n",
    "    \"\"\"Generate preference data for a constitution using stacked LoRA model\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ”„ Generating preferences for {constitution_type} model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load the stacked LoRA model\n",
    "    model, tokenizer = load_stacked_lora_model(constitution_type)\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize preference generator with a judge model\n",
    "    # We can use the same model or a different one for judging\n",
    "    pref_generator = PreferenceGenerator(\n",
    "        feedback_model_name=BASE_MODEL_NAME,  # Use base model for judging\n",
    "        constitution_path=f\"{PROJECT_DIR}/constitutions/{constitution_type}/principles.json\",\n",
    "        constitution_type=constitution_type,\n",
    "        use_soft_labels=True,\n",
    "        min_score_difference=0.1\n",
    "    )\n",
    "    \n",
    "    # Load test prompts\n",
    "    red_team_path = f\"{DATA_PATH}/red_team/sample_red_team.json\"\n",
    "    if not os.path.exists(red_team_path):\n",
    "        print(\"âŒ Red team data not found!\")\n",
    "        return None\n",
    "    \n",
    "    with open(red_team_path, 'r') as f:\n",
    "        red_team_data = json.load(f)\n",
    "    \n",
    "    prompts = [item['prompt'] for item in red_team_data['prompts'][:num_prompts]]\n",
    "    print(f\"Using {len(prompts)} red team prompts\")\n",
    "    \n",
    "    # Generate multiple responses per prompt\n",
    "    def generate_response(prompt):\n",
    "        \"\"\"Generate a response using the model\"\"\"\n",
    "        formatted = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "        inputs = tokenizer(formatted, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # Move inputs to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Vary temperature for diversity\n",
    "            temperature = 0.7 + random.random() * 0.4\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    # Generate preference pairs\n",
    "    print(\"Generating responses and preference pairs...\")\n",
    "    all_pairs = pref_generator.process_dataset(\n",
    "        prompts=prompts,\n",
    "        response_generator=generate_response,\n",
    "        responses_per_prompt=4,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    # Save preference data\n",
    "    output_path = f\"{DATA_PATH}/rl_datasets/{constitution_type}_preferences.jsonl\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    pref_generator.save_preferences(all_pairs, output_path)\n",
    "    \n",
    "    print(f\"âœ… Generated {len(all_pairs)} preference pairs\")\n",
    "    print(f\"   Saved to: {output_path}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Generate for both constitutions\n",
    "deont_pref_path = generate_preferences_for_constitution(\"deontological\", num_prompts=10)\n",
    "conseq_pref_path = generate_preferences_for_constitution(\"consequentialist\", num_prompts=10)\n",
    "\n",
    "print(\"\\nâœ… Preference generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Reward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.reward_model import RewardModelTrainer\n",
    "\n",
    "def train_reward_model(constitution_type, pref_data_path):\n",
    "    \"\"\"Train reward model for a constitution\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸŽ¯ Training reward model for {constitution_type}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    output_dir = f\"{MODEL_PATH}/{constitution_type}/reward_model\"\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = RewardModelTrainer(\n",
    "        model_name=BASE_MODEL_NAME,\n",
    "        constitution_type=constitution_type,\n",
    "        output_dir=output_dir,\n",
    "        use_soft_labels=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train(\n",
    "        train_data_path=pref_data_path,\n",
    "        val_data_path=None,  # Could split data for validation\n",
    "        epochs=2,  # Fewer epochs for reward model\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        learning_rate=CONFIG['learning_rate']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Reward model saved to {output_dir}\")\n",
    "    \n",
    "    # Clean up\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Train reward models if preference data exists\n",
    "if deont_pref_path and os.path.exists(deont_pref_path):\n",
    "    deont_reward_path = train_reward_model(\"deontological\", deont_pref_path)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping deontological reward model training - no preference data\")\n",
    "    deont_reward_path = None\n",
    "\n",
    "if conseq_pref_path and os.path.exists(conseq_pref_path):\n",
    "    conseq_reward_path = train_reward_model(\"consequentialist\", conseq_pref_path)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping consequentialist reward model training - no preference data\")\n",
    "    conseq_reward_path = None\n",
    "\n",
    "print(\"\\nâœ… Reward model training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PPO Training with TRL (Updated for 0.7.0+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "from trl.models import AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_ppo_with_stacked_lora(constitution_type, reward_model_path):\n",
    "    \"\"\"Train model with PPO using constitutional rewards and stacked LoRA\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸš€ PPO training for {constitution_type} model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # PPO configuration for TRL 0.7.0+\n",
    "    ppo_config = PPOConfig(\n",
    "        batch_size=CONFIG['ppo_batch_size'],\n",
    "        mini_batch_size=CONFIG['mini_batch_size'],\n",
    "        gradient_accumulation_steps=CONFIG['gradient_accumulation'],\n",
    "        ppo_epochs=4,\n",
    "        learning_rate=CONFIG['learning_rate'],\n",
    "        optimize_cuda_cache=True,\n",
    "        seed=42,\n",
    "        use_score_scaling=True,\n",
    "        use_score_norm=True,\n",
    "        score_clip=10.0,\n",
    "    )\n",
    "    \n",
    "    # Load the stacked LoRA model\n",
    "    print(\"Loading stacked LoRA model for PPO...\")\n",
    "    base_model, tokenizer = load_stacked_lora_model(constitution_type)\n",
    "    \n",
    "    # Wrap model with value head for PPO\n",
    "    # Note: For stacked LoRA, we need to be careful about the value head\n",
    "    print(\"Adding value head for PPO...\")\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        base_model,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load reward model\n",
    "    print(\"Loading reward model...\")\n",
    "    from src.training.reward_model import RewardModel\n",
    "    reward_model = RewardModel(BASE_MODEL_NAME)\n",
    "    \n",
    "    reward_checkpoint = f\"{reward_model_path}/final_model/reward_model.pt\"\n",
    "    if os.path.exists(reward_checkpoint):\n",
    "        reward_model.load_state_dict(torch.load(reward_checkpoint, map_location=\"cpu\"))\n",
    "        reward_model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            reward_model.cuda()\n",
    "    else:\n",
    "        print(f\"âš ï¸ Reward model checkpoint not found at {reward_checkpoint}\")\n",
    "        print(\"   Using random reward model (not recommended!)\")\n",
    "    \n",
    "    # Create PPO trainer\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    \n",
    "    # Load training prompts\n",
    "    with open(f\"{DATA_PATH}/red_team/sample_red_team.json\", 'r') as f:\n",
    "        red_team_data = json.load(f)\n",
    "    \n",
    "    prompts = [item['prompt'] for item in red_team_data['prompts'][:30]]  # Use 30 prompts\n",
    "    print(f\"Using {len(prompts)} prompts for PPO training\")\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting PPO training...\")\n",
    "    for epoch in range(2):  # 2 epochs\n",
    "        print(f\"\\nEpoch {epoch + 1}/2\")\n",
    "        epoch_rewards = []\n",
    "        \n",
    "        with tqdm(range(0, len(prompts), ppo_config.batch_size), desc=\"PPO Steps\") as pbar:\n",
    "            for batch_start in pbar:\n",
    "                batch_prompts = prompts[batch_start:batch_start + ppo_config.batch_size]\n",
    "                \n",
    "                # Format prompts\n",
    "                formatted_prompts = [f\"Human: {p}\\n\\nAssistant:\" for p in batch_prompts]\n",
    "                \n",
    "                # Tokenize\n",
    "                inputs = tokenizer(\n",
    "                    formatted_prompts,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=CONFIG['max_length'] // 2\n",
    "                )\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "                \n",
    "                # Generate responses\n",
    "                with torch.no_grad():\n",
    "                    response_ids = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=150,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        pad_token_id=tokenizer.pad_token_id\n",
    "                    )\n",
    "                \n",
    "                # Extract response tokens only\n",
    "                response_ids = response_ids[:, inputs['input_ids'].shape[1]:]\n",
    "                \n",
    "                # Get responses text for reward computation\n",
    "                responses = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
    "                \n",
    "                # Compute rewards using reward model\n",
    "                rewards = []\n",
    "                for prompt, response in zip(formatted_prompts, responses):\n",
    "                    # Format for reward model\n",
    "                    full_text = f\"{prompt}{response}\"\n",
    "                    encoding = tokenizer(\n",
    "                        full_text,\n",
    "                        return_tensors=\"pt\",\n",
    "                        truncation=True,\n",
    "                        max_length=CONFIG['max_length']\n",
    "                    )\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        encoding = {k: v.cuda() for k, v in encoding.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        reward_output = reward_model(\n",
    "                            input_ids=encoding['input_ids'],\n",
    "                            attention_mask=encoding['attention_mask']\n",
    "                        )\n",
    "                        reward = reward_output['logits'].squeeze().item()\n",
    "                    \n",
    "                    rewards.append(reward)\n",
    "                \n",
    "                # Convert rewards to tensor\n",
    "                rewards_tensor = torch.tensor(rewards)\n",
    "                if torch.cuda.is_available():\n",
    "                    rewards_tensor = rewards_tensor.cuda()\n",
    "                \n",
    "                # PPO step\n",
    "                stats = ppo_trainer.step(\n",
    "                    queries=inputs['input_ids'],\n",
    "                    responses=response_ids,\n",
    "                    scores=rewards_tensor\n",
    "                )\n",
    "                \n",
    "                # Track rewards\n",
    "                epoch_rewards.extend(rewards)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'reward': f\"{np.mean(rewards):.3f}\",\n",
    "                    'epoch_avg': f\"{np.mean(epoch_rewards):.3f}\"\n",
    "                })\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} - Average Reward: {np.mean(epoch_rewards):.3f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"{MODEL_PATH}/{constitution_type}/rl_cai\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving model to {output_dir}...\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"âœ… RL-CAI model saved to {output_dir}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del reward_model\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Train both models with PPO\n",
    "import numpy as np  # Make sure numpy is imported\n",
    "\n",
    "if deont_reward_path:\n",
    "    deont_rl_path = train_ppo_with_stacked_lora(\"deontological\", deont_reward_path)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping deontological PPO training - no reward model\")\n",
    "    deont_rl_path = None\n",
    "\n",
    "if conseq_reward_path:\n",
    "    conseq_rl_path = train_ppo_with_stacked_lora(\"consequentialist\", conseq_reward_path)\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping consequentialist PPO training - no reward model\")\n",
    "    conseq_rl_path = None\n",
    "\n",
    "print(\"\\nâœ… PPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test RL-CAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rl_model(model_path, constitution_type, prompt):\n",
    "    \"\"\"Test an RL-CAI trained model\"\"\"\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(f\"Loading {constitution_type} RL-CAI model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Format prompt\n",
    "    formatted = f\"Human: {prompt}\\n\\nAssistant:\"\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response = response.split(\"Assistant:\")[-1].strip()\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    clear_gpu_memory()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test on ethical dilemmas\n",
    "test_prompts = [\n",
    "    \"Is it ever acceptable to lie to protect someone's feelings?\",\n",
    "    \"Should I report a friend who is cheating on their taxes?\",\n",
    "    \"Is it okay to break a promise if it helps more people?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RL-CAI MODEL TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for test_prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {test_prompt}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    if deont_rl_path and os.path.exists(deont_rl_path):\n",
    "        print(\"\\nðŸ”µ Deontological RL-CAI:\")\n",
    "        deont_response = test_rl_model(deont_rl_path, \"deontological\", test_prompt)\n",
    "        print(deont_response[:400])\n",
    "    \n",
    "    if conseq_rl_path and os.path.exists(conseq_rl_path):\n",
    "        print(\"\\nðŸŸ¢ Consequentialist RL-CAI:\")\n",
    "        conseq_response = test_rl_model(conseq_rl_path, \"consequentialist\", test_prompt)\n",
    "        print(conseq_response[:400])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Create training summary\n",
    "rl_summary = {\n",
    "    \"training_date\": datetime.datetime.now().isoformat(),\n",
    "    \"base_model\": BASE_MODEL_NAME,\n",
    "    \"architecture\": \"Stacked LoRA: Base Mistral -> HM7B -> CAI\",\n",
    "    \"gpu_type\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "    \"stages_completed\": [\n",
    "        \"preference_generation\",\n",
    "        \"reward_model_training\",\n",
    "        \"ppo_training\"\n",
    "    ],\n",
    "    \"models\": {\n",
    "        \"deontological\": {\n",
    "            \"hm7b_base\": HM7B_PATH,\n",
    "            \"sl_cai\": DEONT_CAI_PATH,\n",
    "            \"reward_model\": deont_reward_path if 'deont_reward_path' in locals() else None,\n",
    "            \"rl_cai\": deont_rl_path if 'deont_rl_path' in locals() else None,\n",
    "            \"preferences\": deont_pref_path if 'deont_pref_path' in locals() else None\n",
    "        },\n",
    "        \"consequentialist\": {\n",
    "            \"hm7b_base\": HM7B_PATH,\n",
    "            \"sl_cai\": CONSEQ_CAI_PATH,\n",
    "            \"reward_model\": conseq_reward_path if 'conseq_reward_path' in locals() else None,\n",
    "            \"rl_cai\": conseq_rl_path if 'conseq_rl_path' in locals() else None,\n",
    "            \"preferences\": conseq_pref_path if 'conseq_pref_path' in locals() else None\n",
    "        }\n",
    "    },\n",
    "    \"config\": CONFIG\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{DRIVE_PROJECT_PATH}/results/rl_training_summary.json\"\n",
    "os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(rl_summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Training summary saved to {summary_path}\")\n",
    "print(\"\\nðŸŽ‰ RL-CAI training complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run 03_evaluation_colab.ipynb to evaluate the models\")\n",
    "print(\"2. Compare responses on divergent ethical dilemmas\")\n",
    "print(\"3. Analyze chain-of-thought reasoning patterns\")\n",
    "print(\"\\nKey paths:\")\n",
    "print(f\"  Deontological RL-CAI: {deont_rl_path if 'deont_rl_path' in locals() else 'Not trained'}\")\n",
    "print(f\"  Consequentialist RL-CAI: {conseq_rl_path if 'conseq_rl_path' in locals() else 'Not trained'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}