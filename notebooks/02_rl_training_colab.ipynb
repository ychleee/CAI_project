{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# RL-CAI Training Notebook\n\nThis notebook implements the Reinforcement Learning stage of Constitutional AI (RL-CAI).\n\nSteps:\n1. Generate preference data using SL-CAI models\n2. Train reward models\n3. Fine-tune with PPO using constitutional feedback\n\n**Important**: Run the cells in order, starting with Section 0!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Prerequisites - Run This First!"
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive for persistent storage\nfrom google.colab import drive\nimport os\nimport sys\n\ndrive.mount('/content/drive')\n\n# Set up paths\nDRIVE_PROJECT_PATH = '/content/drive/MyDrive/Constitutional_AI_Project'\nPROJECT_DIR = '/content/Constitutional_AI_Project'\nGITHUB_REPO = 'https://github.com/ychleee/Constitutional_AI_Project.git'\n\n# Clone or update repository\nif not os.path.exists(PROJECT_DIR):\n    print('\ud83d\udce5 Cloning repository...')\n    !git clone {GITHUB_REPO} {PROJECT_DIR}\nelse:\n    print('\ud83d\udce5 Updating repository...')\n    !cd {PROJECT_DIR} && git pull origin main\n\n# Add project to Python path\nsys.path.append(PROJECT_DIR)\n\n# Install required dependencies\nprint('\ud83d\udce6 Installing dependencies...')\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n!pip install -q peft>=0.6.0 trl>=0.7.0 bitsandbytes>=0.41.0\n!pip install -q einops tensorboard wandb safetensors\n!pip install -q jsonlines pandas numpy scikit-learn matplotlib seaborn tqdm rich\n\n# Create necessary directories in Drive\ndirectories = [\n    f'{DRIVE_PROJECT_PATH}/data/red_team',\n    f'{DRIVE_PROJECT_PATH}/data/helpfulness',\n    f'{DRIVE_PROJECT_PATH}/data/sl_datasets',\n    f'{DRIVE_PROJECT_PATH}/data/rl_datasets',\n    f'{DRIVE_PROJECT_PATH}/data/evaluation',\n    f'{DRIVE_PROJECT_PATH}/models/deontological/sl_cai',\n    f'{DRIVE_PROJECT_PATH}/models/deontological/reward_model',\n    f'{DRIVE_PROJECT_PATH}/models/deontological/rl_cai',\n    f'{DRIVE_PROJECT_PATH}/models/consequentialist/sl_cai',\n    f'{DRIVE_PROJECT_PATH}/models/consequentialist/reward_model',\n    f'{DRIVE_PROJECT_PATH}/models/consequentialist/rl_cai',\n    f'{DRIVE_PROJECT_PATH}/results/sl_training_logs',\n    f'{DRIVE_PROJECT_PATH}/results/rl_training_logs',\n    f'{DRIVE_PROJECT_PATH}/results/evaluation',\n    f'{DRIVE_PROJECT_PATH}/results/figures'\n]\n\nfor dir_path in directories:\n    os.makedirs(dir_path, exist_ok=True)\n\nprint('\u2705 Prerequisites complete!')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os",
    "import sys",
    "import json",
    "import torch",
    "from pathlib import Path",
    "",
    "# Add project to path",
    "PROJECT_DIR = \"/content/Constitutional_AI_Project_v2\"",
    "sys.path.append(PROJECT_DIR)",
    "",
    "# Load configuration from setup, or create default if not found",
    "CONFIG_PATH = '/content/current_config.json'",
    "if os.path.exists(CONFIG_PATH):",
    "    with open(CONFIG_PATH, 'r') as f:",
    "        CONFIG = json.load(f)",
    "    print(f\"\u2705 Loaded existing config for: {CONFIG['model']}\")",
    "else:",
    "    # Default configuration if setup notebook wasn't run",
    "    print(\"\u26a0\ufe0f No config found, creating default configuration...\")",
    "    ",
    "    # Detect GPU and set appropriate config",
    "    if torch.cuda.is_available():",
    "        gpu_name = torch.cuda.get_device_name(0)",
    "        if \"T4\" in gpu_name:",
    "            CONFIG = {",
    "                \"model\": \"EleutherAI/pythia-1.4b\",",
    "                \"quantization\": \"int8\",",
    "                \"batch_size\": 2,",
    "                \"gradient_accumulation\": 8,",
    "                \"max_length\": 512,",
    "                \"lora_r\": 16,",
    "                \"lora_alpha\": 32,",
    "                \"learning_rate\": 2e-5,",
    "                \"fp16\": True,",
    "                \"gradient_checkpointing\": True",
    "            }",
    "            print(f\"\ud83d\udcf1 Detected T4 GPU, using Pythia-1.4B with INT8\")",
    "        elif \"A100\" in gpu_name:",
    "            CONFIG = {",
    "                \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",",
    "                \"quantization\": None,",
    "                \"batch_size\": 4,",
    "                \"gradient_accumulation\": 4,",
    "                \"max_length\": 1024,",
    "                \"lora_r\": 64,",
    "                \"lora_alpha\": 128,",
    "                \"learning_rate\": 1e-4,",
    "                \"bf16\": True,",
    "                \"gradient_checkpointing\": False",
    "            }",
    "            print(f\"\ud83d\ude80 Detected A100 GPU, using Mistral-7B\")",
    "        else:",
    "            # Default to small model for unknown GPU",
    "            CONFIG = {",
    "                \"model\": \"EleutherAI/pythia-1.4b\",",
    "                \"quantization\": \"int8\",",
    "                \"batch_size\": 2,",
    "                \"gradient_accumulation\": 8,",
    "                \"max_length\": 512,",
    "                \"lora_r\": 16,",
    "                \"lora_alpha\": 32,",
    "                \"learning_rate\": 2e-5,",
    "                \"fp16\": True,",
    "                \"gradient_checkpointing\": True",
    "            }",
    "            print(f\"\ud83d\udd27 Using default config for {gpu_name}\")",
    "    else:",
    "        print(\"\u274c No GPU detected\\! This will be very slow.\")",
    "        CONFIG = {",
    "            \"model\": \"EleutherAI/pythia-410m\",",
    "            \"quantization\": None,",
    "            \"batch_size\": 1,",
    "            \"gradient_accumulation\": 1,",
    "            \"max_length\": 256,",
    "            \"lora_r\": 8,",
    "            \"lora_alpha\": 16,",
    "            \"learning_rate\": 2e-5,",
    "            \"fp16\": False,",
    "            \"gradient_checkpointing\": True",
    "        }",
    "    ",
    "    # Save config for other notebooks",
    "    with open(CONFIG_PATH, 'w') as f:",
    "        json.dump(CONFIG, f, indent=2)",
    "    print(f\"\ud83d\udcbe Config saved to {CONFIG_PATH}\")",
    "",
    "# Paths",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/Constitutional_AI_Project_v2'",
    "DATA_PATH = f\"{DRIVE_PROJECT_PATH}/data\"",
    "MODEL_PATH = f\"{DRIVE_PROJECT_PATH}/models\"",
    "",
    "print(f\"Using model: {CONFIG['model']}\")",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Preference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.data.preference_generator import PreferenceGenerator\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport random\n\ndef load_sl_model(model_path, base_model_name):\n    \"\"\"Load SL-CAI trained model\"\"\"\n    # Load base model\n    base_model = AutoModelForCausalLM.from_pretrained(\n        base_model_name,\n        device_map=\"auto\",\n        torch_dtype=torch.float16\n    )\n    \n    # Load LoRA weights\n    model = PeftModel.from_pretrained(base_model, model_path)\n    return model\n\ndef generate_preferences_for_constitution(constitution_type):\n    \"\"\"Generate preference data for a constitution\"\"\"\n    \n    print(f\"\\n\ud83d\udd04 Generating preferences for {constitution_type} model...\")\n    \n    # Load SL-CAI model\n    model_path = f\"{MODEL_PATH}/{constitution_type}/sl_cai\"\n    model = load_sl_model(model_path, CONFIG['model'])\n    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model'])\n    \n    # Initialize preference generator\n    pref_generator = PreferenceGenerator(\n        feedback_model_name=CONFIG['model'],  # Can use same or different model\n        constitution_path=f\"{PROJECT_DIR}/constitutions/{constitution_type}/principles.json\",\n        constitution_type=constitution_type,\n        use_soft_labels=True,\n        min_score_difference=0.1\n    )\n    \n    # Load test prompts\n    with open(f\"{DATA_PATH}/red_team/sample_red_team.json\", 'r') as f:\n        red_team_data = json.load(f)\n    prompts = [item['prompt'] for item in red_team_data['prompts'][:20]]  # Use 20 prompts\n    \n    # Generate multiple responses per prompt\n    def generate_response(prompt):\n        formatted = f\"Human: {prompt}\\n\\nAssistant:\"\n        inputs = tokenizer(formatted, return_tensors=\"pt\", truncation=True).to(\"cuda\")  # Fixed: Move inputs to GPU\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=200,\n                temperature=0.8 + random.random() * 0.4,  # Vary temperature\n                do_sample=True,\n                top_p=0.9,\n                pad_token_id=tokenizer.pad_token_id\n            )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return response.split(\"Assistant:\")[-1].strip()\n    \n    # Generate preference pairs\n    all_pairs = pref_generator.process_dataset(\n        prompts=prompts,\n        response_generator=generate_response,\n        responses_per_prompt=4,\n        show_progress=True\n    )\n    \n    # Save preference data\n    output_path = f\"{DATA_PATH}/rl_datasets/{constitution_type}_preferences.jsonl\"\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    pref_generator.save_preferences(all_pairs, output_path)\n    \n    print(f\"\u2705 Generated {len(all_pairs)} preference pairs\")\n    \n    # Clean up\n    del model\n    torch.cuda.empty_cache()\n    \n    return output_path\n\n# Generate for both constitutions\ndeont_pref_path = generate_preferences_for_constitution(\"deontological\")\nconseq_pref_path = generate_preferences_for_constitution(\"consequentialist\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Reward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.training.reward_model import RewardModelTrainer\n",
    "\n",
    "def train_reward_model(constitution_type, pref_data_path):\n",
    "    \"\"\"Train reward model for a constitution\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83c\udfaf Training reward model for {constitution_type}...\")\n",
    "    \n",
    "    output_dir = f\"{MODEL_PATH}/{constitution_type}/reward_model\"\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = RewardModelTrainer(\n",
    "        model_name=CONFIG['model'],\n",
    "        constitution_type=constitution_type,\n",
    "        output_dir=output_dir,\n",
    "        use_soft_labels=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train(\n",
    "        train_data_path=pref_data_path,\n",
    "        val_data_path=None,  # Could split data for validation\n",
    "        epochs=2,  # Fewer epochs for reward model\n",
    "        batch_size=CONFIG.get('batch_size', 2) * 2,  # Can use larger batch\n",
    "        learning_rate=1e-5\n",
    "    )\n",
    "    \n",
    "    print(f\"\u2705 Reward model saved to {output_dir}\")\n",
    "    return output_dir\n",
    "\n",
    "# Train reward models\n",
    "deont_reward_path = train_reward_model(\"deontological\", deont_pref_path)\n",
    "conseq_reward_path = train_reward_model(\"consequentialist\", conseq_pref_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. PPO Training with TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PPODataset(Dataset):\n",
    "    \"\"\"Dataset for PPO training\"\"\"\n",
    "    def __init__(self, prompts):\n",
    "        self.prompts = prompts\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\"prompt\": self.prompts[idx]}\n",
    "\n",
    "def train_ppo(constitution_type, reward_model_path):\n",
    "    \"\"\"Train model with PPO using constitutional rewards\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83d\ude80 PPO training for {constitution_type} model...\")\n",
    "    \n",
    "    # Load SL-CAI model as starting point\n",
    "    sl_model_path = f\"{MODEL_PATH}/{constitution_type}/sl_cai\"\n",
    "    \n",
    "    # PPO configuration\n",
    "    ppo_config = PPOConfig(\n",
    "        model_name=CONFIG['model'],\n",
    "        learning_rate=1e-5,\n",
    "        batch_size=CONFIG.get('batch_size', 2),\n",
    "        mini_batch_size=1,\n",
    "        gradient_accumulation_steps=CONFIG.get('gradient_accumulation', 8),\n",
    "        ppo_epochs=4,\n",
    "        horizon=10000,\n",
    "        target_kl=0.1,\n",
    "        init_kl_coef=0.2,\n",
    "        seed=42,\n",
    "        use_score_scaling=True,\n",
    "        use_score_norm=True,\n",
    "        score_clip=10.0,\n",
    "    )\n",
    "    \n",
    "    # Load models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['model'])\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with value head for PPO\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        CONFIG['model'],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load reward model\n",
    "    from src.training.reward_model import RewardModel\n",
    "    reward_model = RewardModel(CONFIG['model'])\n",
    "    reward_model.load_state_dict(\n",
    "        torch.load(f\"{reward_model_path}/final_model/reward_model.pt\")\n",
    "    )\n",
    "    reward_model.eval()\n",
    "    reward_model.cuda()\n",
    "    \n",
    "    # Create PPO trainer\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=ppo_config,\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        dataset=None,  # We'll generate on the fly\n",
    "        data_collator=None,\n",
    "    )\n",
    "    \n",
    "    # Load training prompts\n",
    "    with open(f\"{DATA_PATH}/red_team/sample_red_team.json\", 'r') as f:\n",
    "        red_team_data = json.load(f)\n",
    "    prompts = [f\"Human: {item['prompt']}\\n\\nAssistant:\" \n",
    "               for item in red_team_data['prompts'][:50]]\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(2):  # 2 epochs\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        \n",
    "        for batch_start in range(0, len(prompts), ppo_config.batch_size):\n",
    "            batch_prompts = prompts[batch_start:batch_start + ppo_config.batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(\n",
    "                batch_prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Generate responses\n",
    "            with torch.no_grad():\n",
    "                response_ids = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            # Get responses text\n",
    "            responses = tokenizer.batch_decode(\n",
    "                response_ids[:, inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            \n",
    "            # Compute rewards using reward model\n",
    "            rewards = []\n",
    "            for prompt, response in zip(batch_prompts, responses):\n",
    "                # Format for reward model\n",
    "                text = f\"{prompt}{response}\"\n",
    "                encoding = tokenizer(\n",
    "                    text,\n",
    "                    return_tensors=\"pt\",\n",
    "                    truncation=True,\n",
    "                    max_length=512\n",
    "                ).to('cuda')\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    reward = reward_model(\n",
    "                        input_ids=encoding['input_ids'],\n",
    "                        attention_mask=encoding['attention_mask']\n",
    "                    )['logits'].squeeze().item()\n",
    "                \n",
    "                rewards.append(reward)\n",
    "            \n",
    "            # Convert rewards to tensor\n",
    "            rewards_tensor = torch.tensor(rewards).cuda()\n",
    "            \n",
    "            # PPO step\n",
    "            stats = ppo_trainer.step(\n",
    "                queries=inputs['input_ids'],\n",
    "                responses=response_ids[:, inputs['input_ids'].shape[1]:],\n",
    "                scores=rewards_tensor\n",
    "            )\n",
    "            \n",
    "            # Log stats\n",
    "            if batch_start % 10 == 0:\n",
    "                print(f\"  Batch {batch_start}/{len(prompts)}: \"\n",
    "                      f\"Reward: {rewards_tensor.mean().item():.3f}\")\n",
    "    \n",
    "    # Save model\n",
    "    output_dir = f\"{MODEL_PATH}/{constitution_type}/rl_cai\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"\u2705 RL-CAI model saved to {output_dir}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del model\n",
    "    del reward_model\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return output_dir\n",
    "\n",
    "# Train both models with PPO\n",
    "deont_rl_path = train_ppo(\"deontological\", deont_reward_path)\n",
    "conseq_rl_path = train_ppo(\"consequentialist\", conseq_reward_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quick Test of RL-CAI Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_rl_model(model_path, prompt):\n    \"\"\"Test an RL-CAI trained model\"\"\"\n    \n    # Load model and tokenizer\n    model = AutoModelForCausalLM.from_pretrained(\n        model_path,\n        torch_dtype=torch.float16,\n        device_map=\"auto\"\n    )\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n    \n    # Format prompt\n    formatted = f\"Human: {prompt}\\n\\nAssistant:\"\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(\"cuda\")  # Fixed: Move inputs to GPU\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=200,\n            temperature=0.7,\n            do_sample=True,\n            pad_token_id=tokenizer.pad_token_id\n        )\n    \n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return response.split(\"Assistant:\")[-1].strip()\n\n# Test on ethical dilemma\ntest_prompt = \"Is it ever acceptable to lie to protect someone's feelings?\"\n\nprint(f\"Test prompt: {test_prompt}\\n\")\n\nprint(\"Deontological RL-CAI response:\")\ndeont_response = test_rl_model(deont_rl_path, test_prompt)\nprint(deont_response)\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\nprint(\"Consequentialist RL-CAI response:\")\nconseq_response = test_rl_model(conseq_rl_path, test_prompt)\nprint(conseq_response)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Create training summary\n",
    "rl_summary = {\n",
    "    \"training_date\": datetime.datetime.now().isoformat(),\n",
    "    \"base_model\": CONFIG['model'],\n",
    "    \"gpu_type\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
    "    \"stages_completed\": [\n",
    "        \"preference_generation\",\n",
    "        \"reward_model_training\",\n",
    "        \"ppo_training\"\n",
    "    ],\n",
    "    \"models\": {\n",
    "        \"deontological\": {\n",
    "            \"sl_cai\": f\"{MODEL_PATH}/deontological/sl_cai\",\n",
    "            \"reward_model\": deont_reward_path,\n",
    "            \"rl_cai\": deont_rl_path,\n",
    "            \"preferences\": deont_pref_path\n",
    "        },\n",
    "        \"consequentialist\": {\n",
    "            \"sl_cai\": f\"{MODEL_PATH}/consequentialist/sl_cai\",\n",
    "            \"reward_model\": conseq_reward_path,\n",
    "            \"rl_cai\": conseq_rl_path,\n",
    "            \"preferences\": conseq_pref_path\n",
    "        }\n",
    "    },\n",
    "    \"config\": CONFIG\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = f\"{DRIVE_PROJECT_PATH}/results/rl_training_summary.json\"\n",
    "os.makedirs(os.path.dirname(summary_path), exist_ok=True)\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(rl_summary, f, indent=2)\n",
    "\n",
    "print(f\"\u2705 Training summary saved to {summary_path}\")\n",
    "print(\"\\n\ud83c\udf89 RL-CAI training complete!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Run 03_evaluation_colab.ipynb to evaluate the models\")\n",
    "print(\"2. Compare responses on divergent ethical dilemmas\")\n",
    "print(\"3. Analyze chain-of-thought reasoning patterns\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}