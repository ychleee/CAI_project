{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constitutional AI - Setup Notebook\n",
    "\n",
    "This notebook sets up the complete environment for Constitutional AI training on Google Colab.\n",
    "\n",
    "Features:\n",
    "- Automatic GPU detection and configuration\n",
    "- Google Drive persistence for data and models\n",
    "- GitHub repository synchronization\n",
    "- Automatic data download and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites - Run This First!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive for persistent storage\nfrom google.colab import drive\nimport os\nimport sys\n\ndrive.mount('/content/drive')\n\n# Set up paths - UPDATED FOR V2\nDRIVE_PROJECT_PATH = '/content/drive/MyDrive/Constitutional_AI_Project_v2'\nPROJECT_DIR = '/content/Constitutional_AI_Project_v2'\nGITHUB_REPO = 'https://github.com/ychleee/CAI_project.git'\n\n# Clone or update repository\nif not os.path.exists(PROJECT_DIR):\n    print('üì• Cloning repository...')\n    !git clone {GITHUB_REPO} {PROJECT_DIR}\nelse:\n    print('üì• Updating repository...')\n    !cd {PROJECT_DIR} && git pull origin main\n\n# Add project to Python path\nsys.path.append(PROJECT_DIR)\n\n# Install required dependencies\nprint('üì¶ Installing dependencies...')\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n!pip install -q peft>=0.6.0 trl>=0.7.0 bitsandbytes>=0.41.0\n!pip install -q einops tensorboard wandb safetensors\n!pip install -q jsonlines pandas numpy scikit-learn matplotlib seaborn tqdm rich\n\n# Create necessary directories in Drive for V2\ndirectories = [\n    f'{DRIVE_PROJECT_PATH}/data/red_team',\n    f'{DRIVE_PROJECT_PATH}/data/helpfulness',\n    f'{DRIVE_PROJECT_PATH}/data/sl_datasets',\n    f'{DRIVE_PROJECT_PATH}/data/rl_datasets',\n    f'{DRIVE_PROJECT_PATH}/data/evaluation',\n    f'{DRIVE_PROJECT_PATH}/models/deontological/sl_cai',\n    f'{DRIVE_PROJECT_PATH}/models/deontological/reward_model',\n    f'{DRIVE_PROJECT_PATH}/models/deontological/rl_cai',\n    f'{DRIVE_PROJECT_PATH}/models/consequentialist/sl_cai',\n    f'{DRIVE_PROJECT_PATH}/models/consequentialist/reward_model',\n    f'{DRIVE_PROJECT_PATH}/models/consequentialist/rl_cai',\n    f'{DRIVE_PROJECT_PATH}/results/sl_training_logs',\n    f'{DRIVE_PROJECT_PATH}/results/rl_training_logs',\n    f'{DRIVE_PROJECT_PATH}/results/evaluation',\n    f'{DRIVE_PROJECT_PATH}/results/figures'\n]\n\nfor dir_path in directories:\n    os.makedirs(dir_path, exist_ok=True)\n\nprint('‚úÖ Prerequisites complete for v2 project!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to get correct LoRA target modules for each model\n",
    "def get_target_modules(model_name):\n",
    "    \"\"\"Get the correct target modules for LoRA based on model architecture\"\"\"\n",
    "    model_name_lower = model_name.lower()\n",
    "    \n",
    "    if 'pythia' in model_name_lower:\n",
    "        return [\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"]\n",
    "    elif 'mistral' in model_name_lower or 'llama' in model_name_lower:\n",
    "        return [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    elif 'gpt2' in model_name_lower or 'gpt-2' in model_name_lower:\n",
    "        return [\"c_attn\", \"c_proj\"]\n",
    "    elif 'opt' in model_name_lower:\n",
    "        return [\"q_proj\", \"v_proj\", \"k_proj\", \"out_proj\"]\n",
    "    else:\n",
    "        # Default fallback\n",
    "        print(f\"‚ö†Ô∏è Unknown model architecture for {model_name}, using default target modules\")\n",
    "        return [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "# Detect GPU and create appropriate configuration\n",
    "def create_config():\n",
    "    \"\"\"Create configuration based on available GPU\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_info = torch.cuda.get_device_properties(0)\n",
    "        gpu_name = gpu_info.name\n",
    "        gpu_memory = gpu_info.total_memory / 1024**3  # Convert to GB\n",
    "        \n",
    "        print(f\"‚úÖ GPU Available: {gpu_name}\")\n",
    "        print(f\"   Memory: {gpu_memory:.2f} GB\")\n",
    "        \n",
    "        if \"T4\" in gpu_name:\n",
    "            print(\"üì± Using T4 configuration (Free Colab)\")\n",
    "            config = {\n",
    "                \"model\": \"EleutherAI/pythia-1.4b\",\n",
    "                \"quantization\": \"int8\",\n",
    "                \"batch_size\": 2,\n",
    "                \"gradient_accumulation\": 8,\n",
    "                \"max_length\": 512,\n",
    "                \"lora_r\": 16,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"learning_rate\": 2e-5,\n",
    "                \"fp16\": True,\n",
    "                \"bf16\": False,\n",
    "                \"gradient_checkpointing\": True,\n",
    "                \"gpu_type\": \"t4\"\n",
    "            }\n",
    "        elif \"A100\" in gpu_name:\n",
    "            print(\"üöÄ Using A100 configuration (Colab Pro/Pro+)\")\n",
    "            config = {\n",
    "                \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                \"quantization\": None,\n",
    "                \"batch_size\": 4,\n",
    "                \"gradient_accumulation\": 4,\n",
    "                \"max_length\": 1024,\n",
    "                \"lora_r\": 64,\n",
    "                \"lora_alpha\": 128,\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"fp16\": False,\n",
    "                \"bf16\": True,\n",
    "                \"gradient_checkpointing\": False,\n",
    "                \"gpu_type\": \"a100\"\n",
    "            }\n",
    "        elif \"V100\" in gpu_name:\n",
    "            print(\"‚ö° Using V100 configuration (Colab Pro)\")\n",
    "            config = {\n",
    "                \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "                \"quantization\": \"int4\",\n",
    "                \"batch_size\": 2,\n",
    "                \"gradient_accumulation\": 8,\n",
    "                \"max_length\": 768,\n",
    "                \"lora_r\": 32,\n",
    "                \"lora_alpha\": 64,\n",
    "                \"learning_rate\": 1e-4,\n",
    "                \"fp16\": True,\n",
    "                \"bf16\": False,\n",
    "                \"gradient_checkpointing\": True,\n",
    "                \"gpu_type\": \"v100\"\n",
    "            }\n",
    "        else:\n",
    "            print(f\"üîß Using default configuration for {gpu_name}\")\n",
    "            config = {\n",
    "                \"model\": \"EleutherAI/pythia-1.4b\",\n",
    "                \"quantization\": \"int8\",\n",
    "                \"batch_size\": 2,\n",
    "                \"gradient_accumulation\": 8,\n",
    "                \"max_length\": 512,\n",
    "                \"lora_r\": 16,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"learning_rate\": 2e-5,\n",
    "                \"fp16\": True,\n",
    "                \"bf16\": False,\n",
    "                \"gradient_checkpointing\": True,\n",
    "                \"gpu_type\": \"unknown\"\n",
    "            }\n",
    "    else:\n",
    "        print(\"‚ùå No GPU detected! Using CPU configuration (very slow)\")\n",
    "        config = {\n",
    "            \"model\": \"EleutherAI/pythia-410m\",\n",
    "            \"quantization\": None,\n",
    "            \"batch_size\": 1,\n",
    "            \"gradient_accumulation\": 1,\n",
    "            \"max_length\": 256,\n",
    "            \"lora_r\": 8,\n",
    "            \"lora_alpha\": 16,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"fp16\": False,\n",
    "            \"bf16\": False,\n",
    "            \"gradient_checkpointing\": True,\n",
    "            \"gpu_type\": \"cpu\"\n",
    "        }\n",
    "    \n",
    "    # Add target modules based on model\n",
    "    config[\"lora_target_modules\"] = get_target_modules(config[\"model\"])\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Create and save configuration\n",
    "CONFIG = create_config()\n",
    "CONFIG_PATH = '/content/current_config.json'\n",
    "\n",
    "with open(CONFIG_PATH, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Configuration saved to {CONFIG_PATH}\")\n",
    "print(f\"üìä Model: {CONFIG['model']}\")\n",
    "print(f\"üìä LoRA target modules: {CONFIG['lora_target_modules']}\")\n",
    "print(f\"üìä Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"üìä Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Anthropic HH-RLHF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Check if data already exists\n",
    "RED_TEAM_PATH = f\"{DRIVE_PROJECT_PATH}/data/red_team/sample_red_team.json\"\n",
    "HELPFUL_PATH = f\"{DRIVE_PROJECT_PATH}/data/helpfulness/sample_helpful.json\"\n",
    "\n",
    "if os.path.exists(RED_TEAM_PATH) and os.path.exists(HELPFUL_PATH):\n",
    "    print(\"‚úÖ Data already exists in Drive\")\n",
    "else:\n",
    "    print(\"üì• Downloading Anthropic HH-RLHF data...\")\n",
    "    \n",
    "    # Install datasets library if not already installed\n",
    "    !pip install -q datasets\n",
    "    \n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    # Load Anthropic HH-RLHF dataset\n",
    "    print(\"Loading dataset from Hugging Face...\")\n",
    "    dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train[:1000]\")  # Load first 1000 samples\n",
    "    \n",
    "    # Process and create sample datasets\n",
    "    red_team_samples = []\n",
    "    helpful_samples = []\n",
    "    \n",
    "    harmful_keywords = ['hack', 'steal', 'kill', 'hurt', 'illegal', 'weapon', 'drug', 'violence']\n",
    "    \n",
    "    for item in dataset:\n",
    "        if 'chosen' in item:\n",
    "            # Extract prompt from chosen response\n",
    "            text = item['chosen']\n",
    "            if 'Human:' in text:\n",
    "                prompt = text.split('Human:')[1].split('Assistant:')[0].strip()\n",
    "                \n",
    "                # Categorize as harmful or helpful\n",
    "                is_harmful = any(keyword in prompt.lower() for keyword in harmful_keywords)\n",
    "                \n",
    "                if is_harmful and len(red_team_samples) < 100:\n",
    "                    red_team_samples.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"category\": \"red_team\",\n",
    "                        \"source\": \"hh-rlhf\"\n",
    "                    })\n",
    "                elif not is_harmful and len(helpful_samples) < 100:\n",
    "                    helpful_samples.append({\n",
    "                        \"prompt\": prompt,\n",
    "                        \"category\": \"helpful\",\n",
    "                        \"source\": \"hh-rlhf\"\n",
    "                    })\n",
    "    \n",
    "    # Save red team data\n",
    "    red_team_data = {\n",
    "        \"metadata\": {\n",
    "            \"source\": \"Anthropic HH-RLHF\",\n",
    "            \"total_prompts\": len(red_team_samples)\n",
    "        },\n",
    "        \"prompts\": red_team_samples\n",
    "    }\n",
    "    \n",
    "    with open(RED_TEAM_PATH, 'w') as f:\n",
    "        json.dump(red_team_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(red_team_samples)} red team samples to {RED_TEAM_PATH}\")\n",
    "    \n",
    "    # Save helpful data\n",
    "    helpful_data = {\n",
    "        \"metadata\": {\n",
    "            \"source\": \"Anthropic HH-RLHF\",\n",
    "            \"total_prompts\": len(helpful_samples)\n",
    "        },\n",
    "        \"prompts\": helpful_samples\n",
    "    }\n",
    "    \n",
    "    with open(HELPFUL_PATH, 'w') as f:\n",
    "        json.dump(helpful_data, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Saved {len(helpful_samples)} helpful samples to {HELPFUL_PATH}\")\n",
    "\n",
    "print(\"\\nüìä Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\ndef test_model_loading():\n    \"\"\"Test that the model can be loaded with current configuration\"\"\"\n    \n    model_name = CONFIG['model']\n    print(f\"üß™ Testing model loading: {model_name}\")\n    \n    try:\n        # Load tokenizer\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        print(\"‚úÖ Tokenizer loaded\")\n        \n        # Prepare model kwargs\n        model_kwargs = {\n            \"device_map\": \"auto\",\n            \"trust_remote_code\": True\n        }\n        \n        # Add quantization if specified\n        if CONFIG['quantization'] == 'int8':\n            model_kwargs[\"load_in_8bit\"] = True\n        elif CONFIG['quantization'] == 'int4':\n            model_kwargs[\"load_in_4bit\"] = True\n            model_kwargs[\"bnb_4bit_compute_dtype\"] = torch.float16\n        else:\n            if CONFIG.get('bf16'):\n                model_kwargs[\"torch_dtype\"] = torch.bfloat16\n            elif CONFIG.get('fp16'):\n                model_kwargs[\"torch_dtype\"] = torch.float16\n            else:\n                model_kwargs[\"torch_dtype\"] = torch.float32\n        \n        # Load model\n        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n        print(\"‚úÖ Model loaded successfully\")\n        \n        # Test generation\n        test_prompt = \"Hello, how are you today?\"\n        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")  # Fixed: Move inputs to GPU\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                temperature=0.7,\n                do_sample=True,\n                pad_token_id=tokenizer.pad_token_id\n            )\n        \n        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        print(f\"\\nüìù Test generation successful:\")\n        print(f\"Input: {test_prompt}\")\n        print(f\"Output: {response}\")\n        \n        # Clean up\n        del model\n        torch.cuda.empty_cache()\n        print(\"\\n‚úÖ Model test complete. Memory cleared.\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading model: {e}\")\n        print(\"\\nTroubleshooting:\")\n        print(\"1. Check GPU memory: !nvidia-smi\")\n        print(\"2. Try restarting runtime: Runtime -> Restart runtime\")\n        print(\"3. Use smaller model or more aggressive quantization\")\n\n# Test the model\ntest_model_loading()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for training\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        free = torch.cuda.mem_get_info()[0] / 1024**3\n",
    "        total = torch.cuda.mem_get_info()[1] / 1024**3\n",
    "        \n",
    "        print(f\"GPU Memory Status:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Free: {free:.2f} GB\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        return free\n",
    "    return 0\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory\"\"\"\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"‚úÖ GPU memory cleared\")\n",
    "\n",
    "def save_checkpoint_to_drive(model, tokenizer, path):\n",
    "    \"\"\"Save model checkpoint to Google Drive\"\"\"\n",
    "    save_path = f\"{DRIVE_PROJECT_PATH}/models/{path}\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "    print(f\"‚úÖ Checkpoint saved to {save_path}\")\n",
    "\n",
    "# Check GPU memory\n",
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Setup Complete!\\n\")\n",
    "print(\"=\"*50)\n",
    "print(\"Summary:\")\n",
    "print(f\"  GPU: {CONFIG.get('gpu_type', 'unknown').upper()}\")\n",
    "print(f\"  Model: {CONFIG['model']}\")\n",
    "print(f\"  Quantization: {CONFIG.get('quantization', 'None')}\")\n",
    "print(f\"  Project Dir: {PROJECT_DIR}\")\n",
    "print(f\"  Drive Dir: {DRIVE_PROJECT_PATH}\")\n",
    "print(f\"  Config saved: {CONFIG_PATH}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nüìö Next Steps:\")\n",
    "print(\"  1. Run 01_sl_training_colab.ipynb for SL-CAI training\")\n",
    "print(\"  2. Run 02_rl_training_colab.ipynb for RL-CAI training\")\n",
    "print(\"  3. Run 03_evaluation_colab.ipynb to evaluate models\")\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"  - Monitor GPU: !nvidia-smi\")\n",
    "print(\"  - Clear memory: clear_gpu_memory()\")\n",
    "print(\"  - Check memory: get_gpu_memory()\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}